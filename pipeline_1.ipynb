{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9a1e31a-f887-4ed2-90b8-b3d1c64f5365",
   "metadata": {},
   "source": [
    "## PIPELINE 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dc2459-2819-476b-b53a-71aa61c87ccc",
   "metadata": {},
   "source": [
    "This Python script interfaces with MongoDB to aggregate and store aggregated data on hourly bicycle departures from bike-sharing stations, based on the data collected and processed by the project's previous producer and consumer components.\n",
    "\n",
    "After establishing a connection to the MongoDB database via pymongo.MongoClient, I access the sensor_data collection, where the real-time data received and processed by the consumer are stored. The goal is to process this data to calculate aggregated hourly departures by station and store the results in the archived_hourly_departures collection.\n",
    "\n",
    "The first step is to retrieve all documents from the sensor_data collection and convert them into a pandas DataFrame to facilitate analysis. I extract relevant information such as date, station ID, short name, full name, and time from each document's metadata, transforming them into separate columns in the DataFrame.\n",
    "\n",
    "Next, I group the data by date, hour, station ID, and name, aggregating the total number of available bicycles. I apply a series of lambda functions to calculate consecutive differences in the number of available bicycles, thus identifying departures (negative values) and arrivals (positive values) of bicycles at each station on an hourly basis. This allows me to calculate the total number of departures (cnt_departures) and arrivals (count_arrivals) for each station at each specific hour.\n",
    "\n",
    "After processing this data, I reset the DataFrame indices and convert the resulting DataFrame into a JSON format to prepare it for insertion into MongoDB.\n",
    "\n",
    "Finally, I insert the aggregated documents into the archived_hourly_departures collection, where each document represents the hourly aggregated departures per station.\n",
    "This operation is repeated cyclically with an interval of 60 seconds, ensuring that the data are continuously updated. In case of exceptions, the system retries after a short interval of 5 seconds, ensuring the resilience of the archiving process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9eae997-94ae-434c-8826-c34a435f9064",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import pymongo\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077d8a8e-1441-4da9-b52d-6fb3e40a6208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connessione al database MongoDB\n",
    "client = pymongo.MongoClient( \"mongodb://mongoadmin:secret@localhost:27017/\")\n",
    "db = client[\"sensor_data\"]\n",
    "collection = db[\"sensor_data\"]\n",
    "collection2 = db[\"per_station_hourly_departures\"]\n",
    "collection3 = db[\"total_hourly_departures\"]\n",
    "archived_collection = db[\"archived_hourly_departures\"]\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        # Recupera tutti i documenti dalla collection\n",
    "        documents = list(collection.find())\n",
    "\n",
    "        # Converti i documenti in un dataframe\n",
    "        df = pd.DataFrame(documents)\n",
    "        #df['date'] = pd.to_datetime(df['metadata']['date'])  # Converti la colonna 'date' in formato datetime\n",
    "        df['date'] = df['metadata'].apply(lambda x: x['date'])\n",
    "        df['station_id'] = df['metadata'].apply(lambda x: x['station_id'])\n",
    "        df['short_name'] = df['metadata'].apply(lambda x: x['short_name'])\n",
    "        df['name'] = df['metadata'].apply(lambda x: x['name'])\n",
    "        df['ora'] = df['metadata'].apply(lambda x: x['ora'])\n",
    "\n",
    "        #grouped_values = df.groupby(['date', 'ora', 'station_id']).agg({'total_bikes_available': list})\n",
    "        grouped_values = df.groupby(['date', 'ora', 'station_id','name']).agg({'total_bikes_available': list})\n",
    "        df = grouped_values\n",
    "\n",
    "        # Definisci una funzione lambda per calcolare la differenza tra gli elementi consecutivi nella lista\n",
    "        calculate_difference = lambda lst: [lst[i] - lst[i-1] for i in range(1, len(lst))] if len(lst) > 1 else []\n",
    "\n",
    "        # Applica la funzione lambda alla colonna desiderata e assegna i valori alla nuova colonna 'differences'\n",
    "        df['differences'] = df['total_bikes_available'].apply(calculate_difference)\n",
    "\n",
    "        # Definisci una funzione lambda per calcolare la somma dei valori negativi in una lista\n",
    "        count_negatives = lambda lst: sum(x for x in lst if x < 0)\n",
    "\n",
    "        # Applica la funzione lambda alla colonna 'differences' e assegna la somma dei valori negativi alla nuova colonna 'cnt_partenze'\n",
    "        df['cnt_partenze'] = df['differences'].apply(count_negatives)\n",
    "\n",
    "        # Definisci una funzione lambda per calcolare la somma dei valori positivi in una lista\n",
    "        count_positives = lambda lst: sum(x for x in lst if x > 0)\n",
    "\n",
    "        # Applica la funzione lambda alla colonna 'differences' e assegna la somma dei valori positivi alla nuova colonna 'count_arrivi'\n",
    "        df['count_arrivi'] = df['differences'].apply(count_positives)\n",
    "\n",
    "        # Sposta gli indici nel dataframe come colonne\n",
    "        df_reset = df.reset_index()\n",
    "\n",
    "        # Filtra il dataframe reset con gli indici come colonne\n",
    "        row = df_reset\n",
    "\n",
    "        #grouped_row = row.groupby(['date', 'ora', 'station_id']).agg({'cnt_partenze': 'sum'})\n",
    "        grouped_row = row.groupby(['date', 'ora','name']).agg({'cnt_partenze': 'sum'})\n",
    "        grouped_row = grouped_row.reset_index()\n",
    "\n",
    "        # Converti il dataframe in formato JSON\n",
    "        data_json = grouped_row.to_json(orient='records')\n",
    "\n",
    "        # Decodifica il JSON in una lista di documenti\n",
    "        data_list = json.loads(data_json)\n",
    "\n",
    "        # Aggiorna i documenti nella collezione \"per_station_hourly_sales\"\n",
    "        for document in data_list:\n",
    "\n",
    "            archived_collection.insert_one(document)\n",
    "\n",
    "        time.sleep(60)\n",
    "\n",
    "    except Exception as ex:\n",
    "        print(f\"ERRORE : {str(ex)}\")\n",
    "        time.sleep(5)\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30635cc9-a720-4e2b-9d58-dbcf7d1b5b05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3f87e0-d7cc-4978-aa1d-8e6c2f0a361c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
