{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d252f5f-2bf8-469b-b002-cc583b043db0",
   "metadata": {},
   "source": [
    "## CONSUMER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75146f4-ae7c-4b6b-8e69-b461396720ba",
   "metadata": {},
   "source": [
    "This code represents the consumer, It is the component that processes real-time data received from the Kafka topic. It implements an architecture based on Spark for data stream processing and MongoDB for saving the processed information, also integrating HTTP requests to enrich the data with additional information about the stations.\n",
    "\n",
    "Initially, the script makes an HTTP request to retrieve a complete set of bike-sharing station data from a JSON endpoint. Using the Spark Structured Streaming APIs, messages are then read in streaming from the topic, transformed into a pandas DataFrame, which is then passed to the save_to_mongodb function for each data batch.\n",
    "\n",
    "A key function, get_station_info, allows extracting specific details of each station based on the ID, using the previously obtained DataFrame. This step is crucial for enriching the received Kafka messages with contextual information such as the station's capacity, short name, and full name.\n",
    "\n",
    "Furthermore, to ensure efficient data storage, the script implements a function, ensure_ttl_index, which creates a TTL (Time-To-Live) index on a MongoDB collection. This index ensures the automatic deletion of documents after a short period, optimizing disk space management and keeping the data relevant and up-to-date.\n",
    "\n",
    "Finally, the execution of the streaming is managed to remain active indefinitely, until forced termination or the occurrence of an exception, at which point errors are handled, and the Spark session is properly terminated to prevent resource loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93e4a424-2570-4a46-bb00-83fba1b649da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import findspark\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime, timedelta\n",
    "import pytz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9f8e9a-7d55-49f3-9752-0d2a988d8c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/02 17:52:19 WARN Utils: Your hostname, MacBook-Pro-di-Giuseppe.local resolves to a loopback address: 127.0.0.1; using 192.168.200.186 instead (on interface en0)\n",
      "24/03/02 17:52:19 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/panda/mambaforge/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/panda/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/panda/.ivy2/jars\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-918536bf-3846-47cf-b400-b76c747812d5;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;4.0.5 in central\n",
      "\tfound org.mongodb#bson;4.0.5 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.0.5 in central\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.1.2 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.1.2 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.6.0 in central\n",
      "\tfound com.github.luben#zstd-jni;1.4.8-1 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.2 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      ":: resolution report :: resolve 664ms :: artifacts dl 35ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.luben#zstd-jni;1.4.8-1 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.6.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.1.2 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.1.2 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.mongodb#bson;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.0.5 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.2 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   13  |   0   |   0   |   0   ||   13  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-918536bf-3846-47cf-b400-b76c747812d5\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 13 already retrieved (0kB/29ms)\n",
      "24/03/02 17:52:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import findspark\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime, timedelta\n",
    "import pytz\n",
    "\n",
    "# Recupero le informazioni sulle stazioni\n",
    "url2 = 'https://gbfs.capitalbikeshare.com/gbfs/en/station_information.json'\n",
    "response2 = requests.get(url2)\n",
    "data2 = response2.json()\n",
    "df_stations = pd.DataFrame(data2['data']['stations'])\n",
    "\n",
    "def get_station_info(station_id):\n",
    "    station_info = df_stations[df_stations['station_id'] == station_id]\n",
    "    if not station_info.empty:\n",
    "        return station_info.iloc[0].to_dict()\n",
    "    return None\n",
    "\n",
    "def ensure_ttl_index(collection, field=\"expire_at\", ttl_seconds=120):\n",
    "    \"\"\"\n",
    "    Assicura che l'indice TTL sia stato creato sulla collezione per il campo specificato.\n",
    "    Se l'indice non esiste, lo crea con il TTL specificato.\n",
    "    \"\"\"\n",
    "    indexes = collection.index_information()\n",
    "    if not any(index.get('expireAfterSeconds') == ttl_seconds for index in indexes.values()):\n",
    "        collection.create_index([(field, 1)], expireAfterSeconds=ttl_seconds)\n",
    "\n",
    "def save_to_mongodb(batch_df, batch_id):\n",
    "    client = MongoClient(mongo_uri)\n",
    "    db = client[mongo_db]\n",
    "    collection = db[mongo_collection]\n",
    "    \n",
    "    \n",
    "    ensure_ttl_index(collection, \"expire_at\", 120)  # 120 secondi = 2 minuti\n",
    "\n",
    "    documents = []\n",
    "\n",
    "    with open('station_id.txt', 'r') as f:\n",
    "        station_ids = [line.strip() for line in f]\n",
    "\n",
    "    for row in batch_df.collect():\n",
    "        data = row.asDict()\n",
    "        message = json.loads(data[\"message\"])\n",
    "        station_id = message[\"station_id\"]\n",
    "        if station_id in station_ids:\n",
    "            # Impostazioni per il fuso orario e il formato data/ora\n",
    "            fuso_orario_washington = pytz.timezone(\"America/New_York\")\n",
    "            timestamp2 = datetime.now(fuso_orario_washington)\n",
    "            dt_iso_wdc = timestamp2.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            data_washington = timestamp2.strftime(\"%Y-%m-%d\")\n",
    "            ora_washington = timestamp2.strftime(\"%H\")\n",
    "            \n",
    "            # Creazione del timestamp di scadenza per il TTL\n",
    "            expire_at = datetime.utcnow() + timedelta(minutes=2)\n",
    "            \n",
    "            station_info = get_station_info(station_id)\n",
    "\n",
    "            \n",
    "            if station_info:\n",
    "                capacity = station_info[\"capacity\"]\n",
    "                short_name = station_info[\"short_name\"]\n",
    "                name = station_info[\"name\"]\n",
    "                total_bikes_available = message[\"num_bikes_available\"] + message[\"num_ebikes_available\"]\n",
    "\n",
    "                # Calcolo del numero di partenze\n",
    "                departures = capacity - total_bikes_available\n",
    "\n",
    "                # Creazione del documento con il nuovo campo TTL, inclusione di capacity e calcolo delle partenze\n",
    "                values = {\n",
    "                    \"metadata\": {\n",
    "                        \"station_id\": station_id,\n",
    "                        \"short_name\": short_name,\n",
    "                        \"name\": name,\n",
    "                        \"capacity\": capacity,\n",
    "                        \"dt_iso_wdc\": dt_iso_wdc,\n",
    "                        \"date\": data_washington,\n",
    "                        \"ora\": ora_washington\n",
    "                    },\n",
    "                    \"timestamp\": datetime.utcnow(),\n",
    "                    \"expire_at\": expire_at,\n",
    "                    \"total_bikes_available\": total_bikes_available,\n",
    "                    \"departures\": departures  # Aggiunta del campo departures\n",
    "                }\n",
    "                documents.append(values)\n",
    "\n",
    "    if documents:\n",
    "        collection.insert_many(documents)\n",
    "    client.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    findspark.init()\n",
    "    mongo_uri = \"mongodb://mongoadmin:secret@localhost:27017/\"\n",
    "    mongo_db = \"sensor_data\"\n",
    "    mongo_collection = \"sensor_data\"\n",
    "\n",
    "    try:\n",
    "        spark = SparkSession \\\n",
    "            .builder \\\n",
    "            .appName(\"Bike_Share_Data_Processing\") \\\n",
    "            .config(\"spark.mongodb.input.uri\", mongo_uri) \\\n",
    "            .config(\"spark.mongodb.output.uri\", mongo_uri) \\\n",
    "            .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1,org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2\") \\\n",
    "            .getOrCreate()\n",
    "\n",
    "        sc = spark.sparkContext\n",
    "        sc.setLogLevel('ERROR')\n",
    "\n",
    "        df = spark \\\n",
    "            .readStream \\\n",
    "            .format(\"kafka\") \\\n",
    "            .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "            .option(\"subscribe\", \"sensor_data\") \\\n",
    "            .option(\"startingOffsets\", \"latest\") \\\n",
    "            .load() \\\n",
    "            .selectExpr(\"CAST(value AS STRING) as message\")\n",
    "\n",
    "        query = df \\\n",
    "            .writeStream \\\n",
    "            .foreachBatch(save_to_mongodb) \\\n",
    "            .outputMode(\"append\") \\\n",
    "            .start()\n",
    "\n",
    "        query.awaitTermination()\n",
    "    except Exception as ex:\n",
    "        print(f\"ERRORE: {str(ex)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46786bc7-4060-47fd-ba46-0963f3291a29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
