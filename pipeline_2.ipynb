{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15b2ffa2-8188-44fd-ba0f-2be7086cc85d",
   "metadata": {},
   "source": [
    "## PIPELINE 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c74e27-ceca-4916-b347-08ab1fb050b3",
   "metadata": {},
   "source": [
    "This Python script, designed to be executed immediately after the component that stores hourly departures in MongoDB, leverages Spark to further aggregate the data, focusing on creating an aggregated time series per hour and station (aggregated_hourly_departures_per_station).\n",
    "\n",
    "Initially, we configure the MongoDB URI with the necessary authentication credentials to access the sensor_data database and specifically the archived_hourly_departures collection for input and aggregated_hourly_departures_per_station for output. This configuration is essential to ensure that Spark can securely read and write data in MongoDB.\n",
    "\n",
    "The Spark session is initialized with parameters that include the MongoDB URI, database, input and output collections, and the MongoDB Spark Connector package necessary for integration between Spark and MongoDB. This initial setup is crucial to enable the distributed processing of data stored in MongoDB using Spark's capabilities.\n",
    "\n",
    "The core of the script is represented by the process_data function, which reads data from the archived_hourly_departures collection as a Spark DataFrame, aggregates the data by date, hour, and station name, and calculates the number of departures in each combination of date, hour, and station.\n",
    "\n",
    "For each partition of the aggregated DataFrame, we invoke the update_document function, which updates the aggregated_hourly_departures_per_station collection in MongoDB. This function uses the station name as a key for updating, ensuring that the data are organized in an intuitive and easily accessible manner for further analysis.\n",
    "\n",
    "Finally, the infinite loop at the end of the script allows the process_data function to be executed periodically, with a predefined interval of 1800 seconds (30 minutes), ensuring that the aggregated data are continuously updated with the latest information. This approach ensures that data analysis and visualization can benefit from updated and relevant information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e8ccd32-fa4b-4c95-9c39-0f5835e704b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import pymongo\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import abs as pyspark_abs, max as pyspark_max, sum as pyspark_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7296d5a6-b99e-4de1-a93f-a464aee8c088",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/02 17:54:21 WARN Utils: Your hostname, MacBook-Pro-di-Giuseppe.local resolves to a loopback address: 127.0.0.1; using 192.168.200.186 instead (on interface en0)\n",
      "24/03/02 17:54:21 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/panda/mambaforge/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/panda/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/panda/.ivy2/jars\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-127019fd-1586-43c6-9631-494b66d23c41;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;4.0.5 in central\n",
      "\tfound org.mongodb#bson;4.0.5 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.0.5 in central\n",
      ":: resolution report :: resolve 360ms :: artifacts dl 20ms\n",
      "\t:: modules in use:\n",
      "\torg.mongodb#bson;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.0.5 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-127019fd-1586-43c6-9631-494b66d23c41\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 4 already retrieved (0kB/11ms)\n",
      "24/03/02 17:54:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/02 17:54:24 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggiornamento completato.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggiornamento completato.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggiornamento completato.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggiornamento completato.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggiornamento completato.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggiornamento completato.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggiornamento completato.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggiornamento completato.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggiornamento completato.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggiornamento completato.\n"
     ]
    }
   ],
   "source": [
    "# Configurazione dell'URI di MongoDB con autenticazione\n",
    "mongo_uri = \"mongodb://mongoadmin:secret@localhost:27017/sales_data?authSource=admin\"\n",
    "mongo_database = \"sensor_data\"\n",
    "mongo_input_collection = \"archived_hourly_departures\"\n",
    "mongo_output_collection = \"aggregated_hourly_departures_per_station\"\n",
    "\n",
    "# Inizializzazione di SparkSession con autenticazione\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AggregateHourlySalesPerStation\") \\\n",
    "    .config('spark.mongodb.input.uri', mongo_uri) \\\n",
    "    .config('spark.mongodb.input.database', mongo_database) \\\n",
    "    .config('spark.mongodb.input.collection', mongo_input_collection) \\\n",
    "    .config('spark.mongodb.output.uri', mongo_uri) \\\n",
    "    .config('spark.mongodb.output.database', mongo_database) \\\n",
    "    .config('spark.mongodb.output.collection', mongo_output_collection) \\\n",
    "    .config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.12:3.0.1') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "def update_document(iterator):\n",
    "    from pymongo import MongoClient\n",
    "    client = MongoClient(mongo_uri)\n",
    "    db = client[mongo_database]\n",
    "    collection = db[mongo_output_collection]\n",
    "    \n",
    "    for row in iterator:\n",
    "        # Aggiornamento per utilizzare il campo \"name\" al posto di \"station_id\"\n",
    "        query = {\"date\": row.date, \"ora\": row.ora, \"station_name\": row.name}\n",
    "        update = {\"$set\": {\"cnt_partenze\": row.max_abs_cnt_partenze}}\n",
    "        collection.update_one(query, update, upsert=True)\n",
    "    \n",
    "    client.close()\n",
    "\n",
    "def process_data():\n",
    "    try:\n",
    "        df = spark.read.format(\"mongo\").load()\n",
    "\n",
    "        # Calcolo del valore assoluto di cnt_partenze\n",
    "        df_with_abs = df.withColumn(\"abs_cnt_partenze\", pyspark_abs(df[\"cnt_partenze\"]))\n",
    "        \n",
    "        # Aggregazione utilizzando \"name\" al posto di \"station_id\"\n",
    "        max_values = df_with_abs.groupBy(\"date\", \"ora\", \"name\").agg(pyspark_max(\"abs_cnt_partenze\").alias(\"max_abs_cnt_partenze\"))\n",
    "\n",
    "        max_values.foreachPartition(update_document)\n",
    "\n",
    "        print(\"Aggiornamento completato.\")\n",
    "\n",
    "    except Exception as ex:\n",
    "        print(f\"ERRORE: {str(ex)}\")\n",
    "\n",
    "# Reintegrazione del ciclo infinito per l'esecuzione periodica\n",
    "while True:\n",
    "    process_data()\n",
    "    time.sleep(1800)  # Intervallo tra le esecuzioni, ad esempio 60 secondi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628e1f16-69f8-4c96-9a66-3774008eae42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
